name: Review Pull Request with llama.cpp

on:
  pull_request:
    types: [opened, synchronize, reopened]
    branches: ["main"]

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  llama-cpp:
    if: ${{ !contains(github.event.pull_request.labels.*.name, 'skip-ai-review') }}
    continue-on-error: true
    runs-on: ubuntu-latest
    name: llama.cpp
    permissions:
      pull-requests: write
      contents: read
    timeout-minutes: 120
    env:
      HF_MODEL_NAME: Qwen2.5.1-Coder-7B-Instruct-GGUF
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Create temporary directory
        run: mkdir -p /tmp/llama_review

      - name: Process PR description
        id: process_pr
        run: |
          PR_BODY_ESCAPED=$(cat << 'EOF'
          ${{ github.event.pull_request.body }}
          EOF
          )
          PROCESSED_BODY=$(echo "$PR_BODY_ESCAPED" | sed -E 's/\[(.*?)\]\(.*?\)/\1/g')
          echo "$PROCESSED_BODY" > /tmp/llama_review/processed_body.txt

      - name: Fetch branches and output the diff in this step
        run: |
          git fetch origin main:main
          git fetch origin pull/${{ github.event.pull_request.number }}/head:pr-branch
          git diff main..pr-branch > /tmp/llama_review/diff.txt

      - name: Write prompt to file
        id: build_prompt
        run: |
          PR_TITLE=$(echo "${{ github.event.pull_request.title }}" | sed 's/[()]/\\&/g')
          DIFF_CONTENT=$(cat /tmp/llama_review/diff.txt)
          PROCESSED_BODY=$(cat /tmp/llama_review/processed_body.txt)
          echo "<|im_start|>system
          You are an experienced developer reviewing a Pull Request. You focus only on what matters and provide concise, actionable feedback.

          Review Context:
          Repository Name: \"${{ github.event.repository.name }}\"
          Repository Description: \"${{ github.event.repository.description }}\"
          Branch: \"${{ github.event.pull_request.head.ref }}\"
          PR Title: \"$PR_TITLE\"

          Guidelines:
          1. Only comment on issues that:
            - Could cause bugs or security issues
            - Significantly impact performance
            - Make the code harder to maintain
            - Violate critical best practices

          2. For each issue:
            - Point to the specific line/file
            - Explain why it's a problem
            - Suggest a concrete fix
            
          3. Praise exceptional solutions briefly, only if truly innovative

          4. Skip commenting on:
            - Minor style issues
            - Obvious changes
            - Working code that could be marginally improved
            - Things that are just personal preference

          Remember:
          Less is more. If the code is good and working, just say so, with a short message.<|im_end|>
          <|im_start|>user
          This is the description of the pull request:
          \`\`\`markdown
          $PROCESSED_BODY
          \`\`\`

          And here is the diff of the changes, for you to review:
          \`\`\`diff
          $DIFF_CONTENT
          \`\`\`
          <|im_end|>
          <|im_start|>assistant
          ### Overall Summary
          " > /tmp/llama_review/prompt.txt

      - name: Show Prompt
        run: cat /tmp/llama_review/prompt.txt

      - name: Set up Homebrew
        uses: Homebrew/actions/setup-homebrew@master

      - name: Install and cache Homebrew tools
        uses: tecolicom/actions-use-homebrew-tools@v1
        with:
          tools: llama.cpp

      - name: Cache the LLM
        id: cache_llama_cpp
        uses: actions/cache@v4
        with:
          path: ~/.cache/llama.cpp/
          key: llama-cpp-${{ env.HF_MODEL_NAME }}

      - name: Download and cache the LLM
        if: steps.cache_llama_cpp.outputs.cache-hit != 'true'
        run: |
          mkdir -p ~/.cache/llama.cpp/
          curl -L -o ~/.cache/llama.cpp/model.gguf https://huggingface.co/bartowski/${{ env.HF_MODEL_NAME }}/resolve/main/Qwen2.5.1-Coder-7B-Instruct-IQ4_XS.gguf

      - name: Run llama.cpp
        run: |
          llama-server \
            --model ~/.cache/llama.cpp/model.gguf \
            --ctx-size 32768 \
            --threads -1 \
            --predict -1 \
            --temp 0.5 \
            --top-p 0.9 \
            --min-p 0.1 \
            --top-k 0 \
            --cache-type-k q8_0 \
            --cache-type-v q8_0 \
            --flash-attn \
            --port 11434 &

      - name: cURL llama-server to get the completion and timings
        run: |
          DATA=$(jq -n --arg prompt "$(cat /tmp/llama_review/prompt.txt)" '{"prompt": $prompt}')
          echo -e '### Review\n\n' > /tmp/llama_review/response.txt
          # Save the full response to a temporary file
          curl \
            --silent \
            --request POST \
            --url http://localhost:11434/completion \
            --header "Content-Type: application/json" \
            --data "$DATA" > /tmp/llama_review/full_response.json

          # Extract and append content to response.txt
          jq -r '.content' /tmp/llama_review/full_response.json >> /tmp/llama_review/response.txt

          # Pretty print the timings information
          echo "=== Performance Metrics ==="
          jq -r '.timings | to_entries | .[] | "\(.key): \(.value)"' /tmp/llama_review/full_response.json

      - name: Show Response
        run: cat /tmp/llama_review/response.txt

      - name: Find Comment
        uses: peter-evans/find-comment@v3
        id: find_comment
        with:
          issue-number: ${{ github.event.pull_request.number }}
          comment-author: "github-actions[bot]"
          body-includes: "### Review"

      - name: Post or Update PR Review
        uses: peter-evans/create-or-update-comment@v4
        with:
          comment-id: ${{ steps.find_comment.outputs.comment-id }}
          issue-number: ${{ github.event.pull_request.number }}
          body-path: /tmp/llama_review/response.txt
          edit-mode: replace
