name: Review Pull Request with llama.cpp

on:
  pull_request:
    types: [opened, synchronize, reopened]
    branches: ["main"]

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  llama-cpp:
    if: ${{ !contains(github.event.pull_request.labels.*.name, 'skip-ai-review') }}
    continue-on-error: true
    runs-on: ubuntu-latest
    name: llama.cpp
    permissions:
      pull-requests: write
      contents: read
    timeout-minutes: 120
    env:
      LLAMA_CPP_COMMIT: 484d2f31aed34ff9f096e3961125762e81d9b7d6
      HF_MODEL_NAME: Qwen2.5-Coder-1.5B-Instruct-GGUF
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Create temporary directory
        run: mkdir -p /tmp/llama_review

      - name: Process PR description
        id: process_pr
        run: |
          PR_BODY_ESCAPED=$(cat << 'EOF'
          ${{ github.event.pull_request.body }}
          EOF
          )
          PROCESSED_BODY=$(echo "$PR_BODY_ESCAPED" | sed -E 's/\[(.*?)\]\(.*?\)/\1/g')
          echo "$PROCESSED_BODY" > /tmp/llama_review/processed_body.txt

      - name: Fetch branches and output the diff in this step
        run: |
          git fetch origin main:main
          git fetch origin pull/${{ github.event.pull_request.number }}/head:pr-branch
          git diff main..pr-branch > /tmp/llama_review/diff.txt

      - name: Write prompt to file
        id: build_prompt
        run: |
          PR_TITLE=$(echo "${{ github.event.pull_request.title }}" | sed 's/[()]/\\&/g')
          DIFF_CONTENT=$(cat /tmp/llama_review/diff.txt)
          PROCESSED_BODY=$(cat /tmp/llama_review/processed_body.txt)
          echo "<|im_start|>system
          You are an experienced developer reviewing a Pull Request. You focus only on what matters and provide concise, actionable feedback.

          Review Context:
          Repository: \"${{ github.event.repository.name }}\"
          Branch: \"${{ github.event.pull_request.head.ref }}\"
          PR Title: \"$PR_TITLE\"

          Guidelines:
          1. Only comment on issues that:
            - Could cause bugs or security issues
            - Significantly impact performance
            - Make the code harder to maintain
            - Violate critical best practices

          2. For each issue:
            - Point to the specific line/file
            - Explain why it's a problem
            - Suggest a concrete fix
            
          3. Praise exceptional solutions briefly, only if truly innovative

          4. Skip commenting on:
            - Minor style issues
            - Obvious changes
            - Working code that could be marginally improved
            - Things that are just personal preference

          Format your response as the following, only including the sections that are relevant to the Pull Request:

          ### Blocking Issues
          - [File:Line] Issue description + suggested fix

          ### Important Feedback
          - [File:Line] Issue description + suggested fix

          ### Positive Notes
          - Brief note on exceptional solutions

          Remember: Less is more. If the code is good and working, just say so, with a short message.<|im_end|>
          <|im_start|>user
          This is the description of the pull request:
          \`\`\`markdown
          $PROCESSED_BODY
          \`\`\`

          And here is the diff of the changes, for you to review:
          \`\`\`diff
          $DIFF_CONTENT
          \`\`\`
          <|im_end|>
          <|im_start|>assistant
          ### Overall Summary
          " > /tmp/llama_review/prompt.txt

      - name: Show Prompt
        run: cat /tmp/llama_review/prompt.txt

      - name: Cache llama.cpp
        id: cache_llama_cpp
        uses: actions/cache@v4
        with:
          path: ~/.cache/llama.cpp/
          key: llama-cpp-${{ env.LLAMA_CPP_COMMIT }}-${{ env.HF_MODEL_NAME }}

      - name: Clone and build llama.cpp
        if: steps.cache_llama_cpp.outputs.cache-hit != 'true'
        run: |
          git clone https://github.com/ggerganov/llama.cpp.git
          cd llama.cpp
          git checkout ${{ env.LLAMA_CPP_COMMIT }}
          cmake -B build
          cmake --build build --config Release -j --target llama-cli
          mkdir -p ~/.cache/llama.cpp/
          cp ./build/bin/llama-cli ~/.cache/llama.cpp/
          curl -L -o ~/.cache/llama.cpp/model.gguf https://huggingface.co/Qwen/${{ env.HF_MODEL_NAME }}/resolve/main/qwen2.5-coder-1.5b-instruct-q8_0.gguf

      - name: Copy llama.cpp to /usr/local/bin/
        run: cp ~/.cache/llama.cpp/llama-cli /usr/local/bin/

      - name: Run llama.cpp
        run: |
          PROMPT=$(cat /tmp/llama_review/prompt.txt)
          echo -e '### Review\n\n' > /tmp/llama_review/response.txt
          llama-cli \
            --model ~/.cache/llama.cpp/model.gguf \
            --prompt "$PROMPT" \
            --ctx-size 32768 \
            --threads -1 \
            --predict -1 \
            --temp 0.5 \
            --top-p 0.9 \
            --min-p 0.1 \
            --top-k 0 \
            --cache-type-k q8_0 \
            --cache-type-v q8_0 \
            --flash-attn \
            --no-display-prompt >> /tmp/llama_review/response.txt

      - name: Remove [end of text] from response
        run: sed -i 's/\[end of text\]//g' /tmp/llama_review/response.txt

      - name: Show Response
        run: cat /tmp/llama_review/response.txt

      - name: Find Comment
        uses: peter-evans/find-comment@v3
        id: find_comment
        with:
          issue-number: ${{ github.event.pull_request.number }}
          comment-author: "github-actions[bot]"
          body-includes: "### Review"

      - name: Post or Update PR Review
        uses: peter-evans/create-or-update-comment@v4
        with:
          comment-id: ${{ steps.find_comment.outputs.comment-id }}
          issue-number: ${{ github.event.pull_request.number }}
          body-path: /tmp/llama_review/response.txt
          edit-mode: replace
